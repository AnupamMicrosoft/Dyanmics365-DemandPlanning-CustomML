{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "sys.path.append('/anaconda/envs/azureml_py38/lib/python3.8/site-packages')\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1702583749085
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File path\n",
        "file_path = 'your_data.csv'\n",
        "\n",
        "# Read the first two rows for parameters\n",
        "with open(file_path, 'r') as file:\n",
        "    first_line = file.readline().strip().split(',')\n",
        "    second_line = file.readline().strip().split(',')\n",
        "\n",
        "# Create dictionaries for standard and custom parameters\n",
        "standard_params = ['DateGranularity', 'PeriodStartDate', 'PeriodEndDate', 'PredictionTimeWindow', 'MeasureColumn', 'TimeColumn']\n",
        "params = dict(zip(first_line, second_line))\n",
        "custom_params = [param for param in params if param not in standard_params]\n",
        "\n",
        "# Validate and convert standard parameter values\n",
        "params['PeriodStartDate'] = pd.to_datetime(params['PeriodStartDate'])\n",
        "params['PeriodEndDate'] = pd.to_datetime(params['PeriodEndDate'])\n",
        "params['PredictionTimeWindow'] = int(params['PredictionTimeWindow'])\n",
        "\n",
        "# Define the TimeColumn and MeasureColumn based on standard parameters\n",
        "TimeColumn = params['TimeColumn']\n",
        "MeasureColumn = params['MeasureColumn']\n",
        "\n",
        "\n",
        "# Read and print raw dimensions data for debugging\n",
        "data = pd.read_csv(file_path, skiprows=lambda x: x in [0, 1, 2] or pd.isna(x), header=0)\n",
        "print(\"Raw Dimensions Data (first 5 rows):\")\n",
        "print(data.head())\n",
        "# Date range check\n",
        "print(f\"Date range in the file: {data[TimeColumn].min()} to {data[TimeColumn].max()}\")\n",
        "\n",
        "# Convert TimeColumn to datetime\n",
        "data[TimeColumn] = pd.to_datetime(data[TimeColumn], errors='coerce')\n",
        "\n",
        "# Filter data based on the date range\n",
        "if not data[(data[TimeColumn] >= params['PeriodStartDate']) & (data[TimeColumn] <= params['PeriodEndDate'])].empty:\n",
        "    data = data[(data[TimeColumn] >= params['PeriodStartDate']) & (data[TimeColumn] <= params['PeriodEndDate'])]\n",
        "else:\n",
        "    print(\"No data within the specified date range. Please check the PeriodStartDate and PeriodEndDate.\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1702587641122
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ... [previous script sections for reading the file]\n",
        "\n",
        "# Print standard parameters\n",
        "print(\"Standard Parameters:\")\n",
        "for param in standard_params:\n",
        "    print(f\"{param}: {params.get(param)}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print custom parameters\n",
        "print(\"Custom Parameters:\")\n",
        "for param in custom_params:\n",
        "    print(f\"{param}: {params[param]}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print a snapshot of the dimensions data\n",
        "print(\"Dimensions Data (first 5 rows):\")\n",
        "print(data.head())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Standard Parameters:\nDateGranularity: M\nPeriodStartDate: 2023-12-07 00:00:00\nPeriodEndDate: 2025-12-06 00:00:00\nPredictionTimeWindow: 24\nMeasureColumn: _Value\nTimeColumn: _Time\n\n\nCustom Parameters:\nP1: Value1\nP2: Value 2\n\n\nDimensions Data (first 5 rows):\n       _Time  _Value       ProductVariantName WarehouseLocationName  \\\n0 2018-01-01     420  Car Audio Unit-65-Black               Store 2   \n1 2018-01-01     275       Car Audio Unit-500               Store 4   \n2 2018-01-01     239  Car Audio Unit-65-Black               Store 4   \n3 2018-01-01     401       Car Audio Unit-500   Distribution center   \n4 2018-01-01     346       Car Audio Unit-500               Store 1   \n\n   Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  \n0         NaN         NaN         NaN         NaN  \n1         NaN         NaN         NaN         NaN  \n2         NaN         NaN         NaN         NaN  \n3         NaN         NaN         NaN         NaN  \n4         NaN         NaN         NaN         NaN  \n"
        }
      ],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1702583755920
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate data based on DateGranularity\n",
        "if params['DateGranularity'] == 'M':\n",
        "    data[TimeColumn] = data[TimeColumn].dt.to_period('M').dt.to_timestamp()\n",
        "elif params['DateGranularity'] == 'W':\n",
        "    data[TimeColumn] = data[TimeColumn].dt.to_period('W').dt.to_timestamp()\n",
        "# Add conditions for other granularities if required\n",
        "\n",
        "# Feature engineering: Extract year, month, day as separate columns\n",
        "# Assuming 'data' is your DataFrame and 'TimeColumn' holds the correct column name\n",
        "# Confirm the correct column name for the time column\n",
        "print(f\"Time column as per parameters: '{TimeColumn}'\")\n",
        "\n",
        "# Print the actual column names from the DataFrame for verification\n",
        "print(\"Actual column names in DataFrame:\")\n",
        "print(data.columns.tolist())\n",
        "\n",
        "# Check if TimeColumn exists in the DataFrame and convert it to datetime\n",
        "if TimeColumn in data.columns:\n",
        "    data[TimeColumn] = pd.to_datetime(data[TimeColumn], errors='coerce')\n",
        "\n",
        "    # Feature engineering: Extract year, month, day as separate columns\n",
        "    data['Year'] = data[TimeColumn].dt.year\n",
        "    data['Month'] = data[TimeColumn].dt.month\n",
        "    data['Day'] = data[TimeColumn].dt.day\n",
        "\n",
        "    # Now you can drop the original time column as it's been replaced by more specific features\n",
        "    data.drop(TimeColumn, axis=1, inplace=True)\n",
        "else:\n",
        "    print(f\"Column '{TimeColumn}' not found in the data. Please check the DataFrame columns.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Time column as per parameters: '_Time'\nActual column names in DataFrame:\n['_Time', '_Value', 'ProductVariantName', 'WarehouseLocationName', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7']\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1702583182900
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = ['_Value', 'ProductVariantName', 'WarehouseLocationName', 'P1', 'P2']\n",
        "\n",
        "# Drop only if the column exists in the DataFrame\n",
        "columns_to_drop = [col for col in columns_to_drop if col in data.columns]\n",
        "\n",
        "# Now drop the columns\n",
        "X = data.drop(columns_to_drop, axis=1)\n",
        "y = data['_Value']\n",
        "\n",
        "# Ensure that the target variable '_Value' is converted to numeric\n",
        "y = pd.to_numeric(y, errors='coerce')\n",
        "\n",
        "# Drop any rows with NaN in the target variable\n",
        "data.dropna(subset=['_Value'], inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = XGBRegressor()\n",
        "model.fit(X_train, y_train)\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)",
            "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1702583186609
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate future dates for prediction\n",
        "future_dates = pd.date_range(start=params['PeriodEndDate'] + pd.Timedelta(days=1), periods=params['PredictionTimeWindow'], freq=params['DateGranularity'])\n",
        "\n",
        "# Create a dataframe for future predictions\n",
        "future_data = pd.DataFrame({TimeColumn: future_dates})\n",
        "future_data['Year'] = future_data[TimeColumn].dt.year\n",
        "future_data['Month'] = future_data[TimeColumn].dt.month\n",
        "future_data['Day'] = future_data[TimeColumn].dt.day\n",
        "\n",
        "# Add dummy columns for categorical features\n",
        "for col in ['ProductVariantName_dummy', 'WarehouseLocationName_dummy']:\n",
        "    future_data[col] = 0\n",
        "\n",
        "# Ensure the column order in future_data matches the training data\n",
        "future_data = future_data.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Predict future values\n",
        "future_predictions = model.predict(future_data)\n",
        "\n",
        "# Prepare the forecast dataframe\n",
        "forecast = pd.DataFrame({TimeColumn: future_dates, MeasureColumn: future_predictions})\n",
        "forecast['ProductVariantName'] = 'default_variant'  # Replace with actual values or logic\n",
        "forecast['WarehouseLocationName'] = 'default_location'  # Replace with actual values or logic\n",
        "\n",
        "# Export the forecast\n",
        "forecast.to_csv('forecasted_output.csv', index=False)"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1702583250946
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This part is usually done in a Python script or a Jupyter Notebook\n",
        "from azureml.core.environment import Environment\n",
        "from azureml.core.model import InferenceConfig\n",
        "\n",
        "# Create an environment from the environment.yml file\n",
        "env = Environment.from_conda_specification(name=\"sklearn-env\", file_path=\"./dependencies/conda.yaml\")\n",
        "\n",
        "# Define the inference configuration\n",
        "inference_config = InferenceConfig(entry_script=\"./src/score.py\", environment=env)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Warning, azureml-defaults not detected in provided environment pip dependencies. The azureml-defaults package contains requirements for the inference stack to run, and should be included.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1702597519686
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}